#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time    : 2019/4/11 17:23# @Author  : Hanxiaoshun@天谕传说# @Site    : # @File    : news_server_linux_03.py# @Software: PyCharmfrom flask import Flaskfrom flask import requestimport hashlibfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.cluster import DBSCANfrom tqdm import tqdmimport timeimport pandas as pdimport jsonimport osimport nltkfrom nltk.sentiment.vader import SentimentIntensityAnalyzerimport spacyimport en_core_web_smnlp = en_core_web_sm.load()import timeimport osimport codecsimport sysimport osimport jsonfrom collections import Counterfrom nltk.tokenize import WordPunctTokenizerhl = hashlib.md5()app = Flask(__name__)def analysis(fileid=None):    # fileid = "002"    # for i in range(1, len(sys.argv)):    # 	url = sys.argv[i]    # fileid = sys.argv[1]    """        windows    """    status = "ok"    use_second = 0    # originfile = r"D:/data/news_data/"    # fileName = originfile + "javaResultData_" + fileid + ".txt"    # fileFolderRoot = r"D:/data/result_data/"    """        linux    """    originfile = "/data/news_data/"    fileName = originfile + "javaResultData_" + fileid + ".txt"    fileFolderRoot = "/data/result_data/"    size = os.path.getsize(fileName)    if size == 0:        print('文件是空的，file is empty')        status = "file is empty"        use_second = 0        return status, use_second    else:        print('文件不是空的,继续解析，file is not empty')    try:        if os.path.exists(fileFolderRoot):            pass        else:            os.mkdir(fileFolderRoot)        start = time.time()        print('start time', str(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))))        data = load_data([fileName])        log = open('./err.txt', 'w')        print("fileid::" + fileid)        os.makedirs(fileFolderRoot + 'result_' + fileid + '/', exist_ok=True)        # print('len(data):::',len(data))        _ids = []        titles = []        raw_contents = []        sites = []        dates = []        sentiment = SentimentIntensityAnalyzer()  # Give a sentiment intensity score to sentences. 对句子进行情感强度评分        # 在处理大规模数据时或者需要迭代多次耗时很长的任务时，可以利用Python tqdm模块来显示任务进度条。常用的是tqdm模块中的tqdm和trange        data_num = 0        # data = collection.find({}).limit(100)        for item in tqdm(data):            try:                data_num += 1                # print(f"注入第 {data_num} 条数据")                raw_contents.append(item["s_cont"])                _ids.append(item['_id'])                titles.append(item['s_title'])                sites.append(parse_url(item['o_url']))                t = time.localtime(int(item['s_pt']) // 1000)                dates.append(time.strftime('%Y%m%d', t))            except Exception as e:                log.write(json.dumps(item))                log.write('\n')                log.write(str(e))        log.close()        """        python 文本特征提取主要两个api        CountVectorizer：            只考虑词汇在文本中出现的频率        TfidfVectorizer：            除了考量某词汇在文本出现的频率，还关注包含这个词汇的所有文本的数量            能够削减高频没有意义的词汇出现带来的影响, 挖掘更有意义的特征相比之下，文本条目越多，Tfid的效果会越显著"""        vectorizer = TfidfVectorizer(analyzer=parse_content, min_df=10, max_df=0.7)        # fit_transform()先拟合数据，再标准化        contents = vectorizer.fit_transform(raw_contents)        # cluster = DBSCAN(eps=1,min_samples=5) #Density-Based Spatial Clustering of Applications with Noise 基于密度的噪声应用空间聚类        # ys = cluster.fit_predict(contents) #对X执行群集并返回群集标签。        # n = len(set(ys))        # title_cluster = [[] for i in range(n)]        #        # for i,y in enumerate(ys):        # 	if y != -1:        # 		title_cluster[y].append(i)        # clusters01 = []        # # with open(fileFolderRoot+'result_'+fileid+'/cluster_result_5.txt','w',encoding='utf-8') as file:        # for i, cluster in enumerate(sorted(title_cluster, key=lambda x: -len(x))):        # 	# file.write('cluster%d\t nums: %d\n'%(i,len(cluster)))        # 	tmp01 = {'cluster': i, 'len': len(cluster)}        # 	list01 = []        # 	for j in cluster:        # 		list01.append(_ids[j])        # 	# file.write("%s:\t%s:\t%s" % (sites[j], titles[j], _ids[j]))        # 	# file.write(",%s" % (_ids[j]))        # 	tmp01['title'] = titles[0]        # 	tmp01['ids'] = ','.join(list01)        # 	clusters01.append(str(tmp01))        # 		# file.write('\n\n')        # # str_clusters01 = '\n'.join(clusters01)        # with open(fileFolderRoot+'result_' + fileid + '/cluster_result_5.txt', 'w', encoding='utf-8') as file:        # 	for i_str in clusters01:        # 		file.write("%s" % i_str)        # 		file.write('\n\n')        cluster = DBSCAN(eps=1, min_samples=20)        ys = cluster.fit_predict(contents)        n = len(set(ys))        title_cluster = [[] for i in range(n)]        for i, y in enumerate(ys):            if y != -1:                title_cluster[y].append(i)        # clusters02 = []        # with open(fileFolderRoot+'result_'+fileid+'/cluster_result_20.txt','w',encoding='utf-8') as file:        # for i, cluster in enumerate(sorted(title_cluster, key=lambda x: -len(x))):        # 	tmp01 = {"cluster": i, "len": len(cluster)}        # 	list01 = []        # 	for j in cluster:        # 		list01.append(_ids[j])        # 	# file.write("%s:\t%s:\t%s" % (sites[j], titles[j], _ids[j]))        # 	# file.write(",%s" % (_ids[j]))        # 	tmp01["title"] = titles[0]        # 	tmp01["ids"] = ','.join(list01)        # 	json_info = json.dumps(tmp01)        # 	clusters02.append(str(json_info))        with open(fileFolderRoot + 'result_' + fileid + '/cluster_result_20.txt', 'w', encoding='utf-8') as file:            # title_index = 0            for i, cluster in enumerate(sorted(title_cluster, key=lambda x: -len(x))):                num_index = 0                title_content = ''                for j in cluster:                    # if j == len(cluster) - 1:                    # 	file.write("%s\n" % (_ids[j]) + 'cluster: %d \t nums: %d \t title: %s\n' % (i, len(cluster), titles[j]))                    # else:                    file.write("%s\n" % (_ids[j]))                    if num_index == 0:                        title_content = titles[j]                    else:                        pass                    num_index = num_index + 1                        # file.write('cluster%d\t%d\t%s\n' % (i, len(cluster), titles[j]))                # file.write('cluster%d\t%d\t%s\n' % (i, len(cluster), titles[0]))                file.write('cluster%d\t%d\t%s\n' % (i, len(cluster), title_content))                # title_index = title_index + num_index - 1        score_counter = {}        for date, content in zip(dates, raw_contents):            if date not in score_counter:                score_counter[date] = {'pos': 0, 'neg': 0}            """            Return a float for sentiment strength based on the input text.            Positive values are positive valence, negative value are negative            valence.            基于输入文本返回情感强度浮动。             正值是正价，负值是负价。原子价。            """            score = sentiment.polarity_scores(content)            if score['pos'] > score['neg']:                # if score['pos']>score['neu']:                score_counter[date]['pos'] += 1            # else:            # score_counter[date]['neu']+=1            else:                # if score['neg']>score['neu']:                score_counter[date]['neg'] += 1        # else:        # score_counter[date]['neu']+=1        dataframe = pd.DataFrame.from_dict(score_counter, orient='index')        dataframe.to_csv(fileFolderRoot + 'result_' + fileid + '/score_stat_2.csv')        # counter = Counter(sites)        # with open(fileFolderRoot+'result_'+fileid+'/sites_stat.txt','w',encoding='utf-8') as file:        # 	for site,num in sorted(counter.items(),key=lambda x:-x[1]):        # 		file.write('%s\t%d\n'%(site,num))        # domains = []        # for site in sites:        # 	domains.append(site.split('.')[-1])        # domains = Counter(domains)        # with open(fileFolderRoot+'result_'+fileid+'/domains_stat.txt','w',encoding='utf-8') as file:        # 	for domain,num in sorted(domains.items(),key=lambda x:-x[1]):        # 		file.write('%s\t%d\n'%(domain,num))        with open(fileFolderRoot + 'result_' + fileid + '/dates_stat.txt', 'w') as file:            for date, num in sorted(Counter(dates).items(), key=lambda x: int(x[0])):                file.write('%s\t%d\n' % (date, num))        ners = {}        for content in tqdm(raw_contents[:]):            for ent in nlp(content).ents:                if ent.label_ in ners:                    ners[ent.label_].append(ent.text)                else:                    ners[ent.label_] = [ent.text]            # print(ent.text,ent.label_)        # gpe_file = open(fileFolderRoot+'result_'+fileid+'/GPE_stat.txt','w',encoding='utf-8')        org_file = open(fileFolderRoot + 'result_' + fileid + '/ORG_stat.txt', 'w', encoding='utf-8')        loc_file = open(fileFolderRoot + 'result_' + fileid + '/LOC_stat.txt', 'w', encoding='utf-8')        per_file = open(fileFolderRoot + 'result_' + fileid + '/PER_stat.txt', 'w', encoding='utf-8')        # file_map = {'GPE':gpe_file,'ORG':org_file,'LOC':loc_file,'PERSON':per_file}        file_map = {'ORG': org_file, 'LOC': loc_file, 'PERSON': per_file}        for tag, ents in ners.items():            if tag in ['ORG', 'LOC', 'PERSON']:                # file.write('%s\n'%tag)                for ent, num in sorted(Counter(ents).items(), key=lambda x: -x[1]):                    file_map[tag].write('%s\t%d\n' % (ent, num))                file_map[tag].close()            # file.write('\n\n\n')        end = time.time()        use_second = end - start        use_minute = use_second / 60        print('end time', str(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))))        print('解析', data_num, '条数据：：用时：：', use_second, ':秒', ' ', use_minute, ':分')        print('is over')        return status, use_second    except Exception as e:        print(str(e))        return str(e), use_seconddef load_data(filenames):    log = open('log.txt', 'w', encoding='utf-8')    data = []    urls = set([])    if not isinstance(filenames, list):        filenames = [filenames]    for filename in filenames:        with open(filename, encoding='utf-8', errors='ignore') as file:            line_num = 0            for line in file:                line_num = line_num + 1                line = line.strip()                try:                    # print(f"整理第 {line_num} 条数据")                    if not line[-1] == ',':                        # line = json.loads(line, strict=False)                        line = json.loads(line)                    else:                        line = json.loads(line[:-1])                    data.append(line)                except Exception as e:                    print(line_num)                    print(str(e))                    log.write(line)                    log.write('\n')    print("len::::", data.__len__())    return datadef parse_url(url):    return url.split('/')[2]def parse_content(content):    return WordPunctTokenizer().tokenize(content)def stat_site(urls):    counter = Counter()    for url in urls:        counter.update([parse_url(url)])    return sorted(counter.items(), key=lambda x: -x[1])"""首页"""@app.route('/', methods=['GET'])def signin_form():    return '''新闻分析              <form action="/analysis" method="post" target=“_blank”>              <p><input name="newsid" placeholder="请输入要分析的文件ID"></p>              <p><button type="submit">开始分析</button></p>              </form>              <hr>                            '''"""访问路径"""@app.route('/analysis', methods=['POST'])def langidcheckStr():    result = {}    # 需要从request对象读取表单内容：    newsid = request.form['newsid']    # return "success"    #    # return json.dumps(result, ensure_ascii=False)    statusx, usetimex = analysis(newsid)    if "ok" in statusx:        result["status"] = "success"        result["usetime"] = str(usetimex)        print("result::" + str(result))        return json.dumps(result, ensure_ascii=False)    else:        result["status"] = statusx        result["usetime"] = 0        return json.dumps(result, ensure_ascii=False)if __name__ == '__main__':    """    Flask 单例微服务启动    """    app.run(host='0.0.0.0', port=45732)    app.run()    # tmpcheckurlArray = ['http://xn--gmq25jl5h1znj5doy1e2pj5ka.xn--zfr164b.cn/']    # tmpcheckurlArray = ['https://www.163.com/']    # mainController.dealFunction(tmpcheckurlArray)